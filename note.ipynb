{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade ipywidgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703616bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from json import load, dump\n",
    "from random import shuffle\n",
    "from torch import tensor\n",
    "\n",
    "from os import path as px\n",
    "from transformers import RobertaConfig, GPT2Config, AutoModelWithLMHead, RobertaForMaskedLM, GPT2LMHeadModel\n",
    "from transformers import RobertaTokenizerFast, GPT2TokenizerFast, DataCollatorForLanguageModeling, TrainingArguments\n",
    "from torch import cuda\n",
    "from transformers import DefaultFlowCallback, ProgressCallback\n",
    "from transformers.trainer_callback import TrainerState, TrainerControl, TrainingArguments, IntervalStrategy\n",
    "\n",
    "from transformers import pipeline\n",
    "from random import randint\n",
    "from torch import randint as torch_rand\n",
    "\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df3a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "for c in range(0,cuda.device_count()):\n",
    "    print(cuda.get_device_name(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66024d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, jpath):\n",
    "        with open(jpath, \"r\", encoding=\"utf-8\") as jf:\n",
    "            self.examples = load(jf)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f76344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Settings # #\n",
    "\n",
    "# paths\n",
    "train_path = \"/mini_json_dataset/mini_train.json\"\n",
    "dev_path = \"/mini_json_dataset/mini_dev.json\"\n",
    "tokenizer_path = \"/tokenizer/tokenizer.json\"\n",
    "\n",
    "# model\n",
    "model_type = \"roberta\"  # gpt2 roberta\n",
    "pretrained_model = None\n",
    "\n",
    "# training parameters\n",
    "model_folder = \"saved\"\n",
    "epochs = 4\n",
    "learning_rate = 0.0001\n",
    "decay = 0.01\n",
    "batch_size = 16\n",
    "dev_batch_size = 16\n",
    "save_steps = 32 #8192\n",
    "eval_steps = 64 #4096\n",
    "save_total_limit = 1\n",
    "warmup_steps = 5  # 500\n",
    "\n",
    "# tokenizer dependent\n",
    "bos_token = 50259\n",
    "eos_token = 50260\n",
    "\n",
    "# misc\n",
    "encoded_file_keyword = \"_encoded_\"\n",
    "\n",
    "# model config for gpt2\n",
    "gpt2_large_config = GPT2Config(\n",
    "        attn_pdrop=0.1,\n",
    "        bos_token_id=bos_token,\n",
    "        embd_pdrop=0.1,\n",
    "        eos_token_id=eos_token,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_epsilon=1e-05,\n",
    "        model_type=\"gpt2\",\n",
    "        n_ctx=1024,\n",
    "        n_embd=1280,\n",
    "        n_head=20,\n",
    "        n_layer=36,\n",
    "        n_positions=1024,\n",
    "        resid_pdrop=0.1,\n",
    "        summary_activation=None,\n",
    "        summary_first_dropout=0.1,\n",
    "        summary_proj_to_labels=True,\n",
    "        summary_type=\"cls_index\",\n",
    "        summary_use_proj=True,\n",
    "        task_specific_params={\n",
    "            \"text-generation\":\n",
    "            {\n",
    "              \"do_sample\": True,\n",
    "              \"max_length\": 50\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# model config for roberta\n",
    "roberta_large_config = RobertaConfig(\n",
    "        max_position_embeddings=514,\n",
    "        num_attention_heads=16,  # 16\n",
    "        num_hidden_layers=24,  # 24\n",
    "        type_vocab_size=1,\n",
    "\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        bos_token_id=bos_token,\n",
    "        eos_token_id=eos_token,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        hidden_size=1024,\n",
    "        initializer_range=0.02,\n",
    "        intermediate_size=4096,\n",
    "        layer_norm_eps=1e-05,\n",
    "    )\n",
    "\n",
    "output_from_model = True\n",
    "\n",
    "# Device initialization\n",
    "device = \"cuda:0\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model initialization\n",
    "if pretrained_model:\n",
    "    model = AutoModelWithLMHead.from_pretrained(pretrained_model)\n",
    "else:\n",
    "    # Model configuration\n",
    "    if model_type == \"gpt2\":\n",
    "\n",
    "        tokenizer = GPT2TokenizerFast(tokenizer_file=tokenizer_path, padding=False, pad_token=\"a\")\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False,\n",
    "        )\n",
    "\n",
    "        model_config = gpt2_large_config\n",
    "        model_config.vocab_size = tokenizer.vocab_size\n",
    "        model = GPT2LMHeadModel.from_config(model_config)\n",
    "\n",
    "    elif model_type == \"roberta\":\n",
    "\n",
    "        tokenizer = RobertaTokenizerFast(tokenizer_file=tokenizer_path,\n",
    "                                         pad_token=\"<pad>\", unk_token=\"<unk>\", mask_token=\"<mask>\")\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            mlm=True,\n",
    "            mlm_probability=0.15,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        model_config = roberta_large_config\n",
    "        model_config.vocab_size = tokenizer.vocab_size\n",
    "        model = RobertaForMaskedLM(config=model_config)\n",
    "\n",
    "# Training args fill\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=decay,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=dev_batch_size,\n",
    "    save_steps=save_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    warmup_steps=warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c20009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some examples\n",
    "fill_test_examples = [\n",
    "    \"Ana ide u <mask>.\",\n",
    "    \"Osnovna <mask> Vuk Karadžić\",\n",
    "    \"Kupio sam dva <mask> i mleko.\"\n",
    "]\n",
    "\n",
    "default_gen_input = \"\"\n",
    "\n",
    "\n",
    "def fill_examples(mod, tok):\n",
    "    # Create a Fill mask pipeline\n",
    "    fill_mask = pipeline(\n",
    "        \"fill-mask\",\n",
    "        model=mod,\n",
    "        tokenizer=tok,\n",
    "        device=device,\n",
    "        top_k=3\n",
    "    )\n",
    "    examples = []\n",
    "    for example in fill_test_examples:\n",
    "        examples.append([x[\"sequence\"] for x in fill_mask(example)])\n",
    "    return examples\n",
    "\n",
    "\n",
    "def generate(model, context, length=20, temperature=0.75):\n",
    "\n",
    "    encoded_input = context.to(device)\n",
    "    output = model.generate(\n",
    "        **encoded_input,\n",
    "        bos_token_id=randint(1, 50000),\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        max_length=length,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=3,\n",
    "        # top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=0\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def generatetion_test(mod, tok, samples=3, length=24, context=default_gen_input, temp=0.75):\n",
    "\n",
    "    outs = []\n",
    "    if context == \"\":\n",
    "        tokens = torch_rand(low=260, high=52000, size=(1,))\n",
    "        context = tok.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    context = tok(context, return_tensors=\"pt\")\n",
    "    cl = context.data[\"input_ids\"].size()[1]\n",
    "\n",
    "    for x in range(samples):\n",
    "        output = generate(mod, context=context, length=length+cl, temperature=temp)\n",
    "\n",
    "        decoded_output = []\n",
    "        for sample in output:\n",
    "            sample = sample[cl:]\n",
    "            decoded_output.append(tokenizer.decode(sample, skip_special_tokens=True))\n",
    "\n",
    "        outs.append(\"\".join(decoded_output))\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "def test(mod, tok=tokenizer):\n",
    "    if model_type == \"roberta\":\n",
    "        return fill_examples(mod, tok)\n",
    "    elif model_type == \"gpt2\":\n",
    "        return generatetion_test(mod, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6821d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDefaultFlowCallback(DefaultFlowCallback):\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Log\n",
    "        if state.global_step == 1 and args.logging_first_step:\n",
    "            control.should_log = True\n",
    "        if args.logging_strategy == IntervalStrategy.STEPS and state.global_step % args.logging_steps == 0:\n",
    "            control.should_log = True\n",
    "\n",
    "        # Evaluate\n",
    "        if (\n",
    "            args.evaluation_strategy == IntervalStrategy.STEPS\n",
    "            and state.global_step % args.eval_steps == 0\n",
    "            and args.eval_delay <= state.global_step\n",
    "        ):\n",
    "            control.should_evaluate = True\n",
    "\n",
    "        # Save\n",
    "        if (\n",
    "            args.save_strategy == IntervalStrategy.STEPS\n",
    "            and args.save_steps > 0\n",
    "            and state.global_step % args.save_steps == 0\n",
    "        ):\n",
    "            control.should_save = True\n",
    "            examples = test(kwargs[\"model\"])\n",
    "            examples = [e for ee in examples for e in ee]\n",
    "            with open(model_folder + \"/log\", \"a+\", encoding=\"utf-8\") as lf:\n",
    "\n",
    "                lf.write(\"\\t\".join(examples))\n",
    "                lf.write(\"\\n\")\n",
    "\n",
    "        # End training\n",
    "        if state.global_step >= state.max_steps:\n",
    "            control.should_training_stop = True\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebde8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JsonDataset(train_path)\n",
    "eval_dataset = JsonDataset(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7ea3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1d0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_from_model:\n",
    "    trainer.remove_callback(DefaultFlowCallback)\n",
    "    trainer.add_callback(CustomDefaultFlowCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihailo.skoric.rgf/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 349073\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10912\n",
      "  Number of trainable parameters = 355407957\n",
      "/usr/local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='10912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   20/10912 00:16 < 2:43:59, 1.11 it/s, Epoch 0.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0baf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
